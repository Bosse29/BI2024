---
title: '2'
author: "GhazalArzanian, Bosse Behrens"
date: "2024-11-12"
output: html_document
---
First install the packages if not already done so.
```{r}
#install.packages("sf", "tigris", "dplyr", "ggplot2", "fastDummies")
```
We load the needed packages.
```{r}
library(sf)
library(tigris)
library(dplyr)
library(ggplot2)
library(fastDummies)
```


##  Data Description Report

### Attribute types and Statistical Properties

First we look at the head of the data to see how the data looks like and get a first impression. We check the structure and types. As we can see all the variables are numbers num or int. In the summary we get an overview over some statistical aspects.

```{r}
df <- read.csv("miami-housing.csv")
data<-df
str(data)
summary(data)

```
In this step we want to look at variables that have a high (more than 0.5 absolute) correlation with each other.
```{r}

correlations <- cor(data, use = "complete.obs")

high_corr <- which(abs(correlations) > 0.5 & abs(correlations) < 1, arr.ind = TRUE)

high_corr_pairs <- data.frame(
  Column1 = rownames(correlations)[high_corr[, 1]],
  Column2 = colnames(correlations)[high_corr[, 2]],
  Correlation = correlations[high_corr]
)

high_corr_pairs <- high_corr_pairs[!duplicated(t(apply(high_corr_pairs[, 1:2], 1, sort))), ]

print(high_corr_pairs)

```

### Data quality aspects and visual Exploration

The data is clean and we dont have any missing data.

```{r}
colSums(is.na(data))
```

We have identified a large outleir in SPEC_FEAT_VALUE and want to inspect it.
```{r}
top_10_spec_feat <- data %>%
  arrange(desc(SPEC_FEAT_VAL)) %>%
  slice_head(n = 10)

print(top_10_spec_feat)
```

We use histograms to look that the distribution of the varibles.

```{r}
numeric_cols <- data[sapply(data, is.numeric)]
numeric_cols$PARCELNO <- NULL
barplot_columns <- c("structure_quality", "month_sold", "avno60plus", "age")

for (col in names(numeric_cols)) {
  if (col %in% barplot_columns) {
  
    freq_table <- table(numeric_cols[[col]])
    barplot(freq_table, main = paste("Frequency of values in", col), xlab = col, ylab = "Frequency", 
            col = "lightblue", border = "black")
  } else {
    hist(numeric_cols[[col]], main = paste("Histogram of", col), xlab = col, col = "lightblue", border = "black")
  }
}
```

Now we further explore the relationships between variables we have identified with high correlations.
```{r}
ggplot(data, aes(x = TOT_LVG_AREA, y = SPEC_FEAT_VAL)) +
  geom_point(color = "blue", size = 2, alpha = 0.5) +
  geom_smooth(method = "lm", color = "red", se = FALSE) +
  labs(title = "Total Living Area vs Special Feature Value with Trend Line",
       x = "Total Living Area",
       y = "Special Feature Value") +
  theme_minimal()


```
```{r}
ggplot(data, aes(x = TOT_LVG_AREA, y = SALE_PRC)) +
  geom_point(color = "blue", size = 2, alpha = 0.5) +
  geom_smooth(method = "lm", color = "red", se = FALSE) +
  labs(title = "Scatterplot of Total Living Area vs Sale Price",
       x = "Total Living Area",
       y = "Sale Price") +
  theme_minimal()

```
```{r}
data$avno60plus <- as.factor(data$avno60plus)

plot(data$avno60plus, data$SALE_PRC,
     xlab = "Airplane noise exceeding acceptable Limit?",
     ylab = "Sale Price",
     main = "Boxplot of Noise dummy",
     pch = 19, col = rgb(0, 0, 1, alpha = 0.5),
     outline = FALSE)
```

```{r}
data$structure_quality <- as.factor(data$structure_quality)

plot(data$structure_quality, data$SALE_PRC,
     xlab = "Structure Quality Grades",
     ylab = "Sale Price",
     main = "Scatterplot of Structure Quality vs Sale Price",
     pch = 19, col = rgb(0, 0, 1, alpha = 0.5))

```

## Data Preparation report 

### Preprocessing

#### Handling Duplicates in PARCELNO

We start by handling the duplicate sin the unique identifier ID PARCELNO.
As we see there are some rows that have the same parcelno numbers. It means they are different records for the same property. Since the data is for one year only we will consider the later month and delete the earlier months, because we want to predict future data and newer entries are therefore more important.
```{r}
duplicate_rows <- data[duplicated(data$PARCELNO) | duplicated(data$PARCELNO, fromLast = TRUE), ]
duplicate_rows

```
We check if the later month always
As we see the higher price is not always for the last month.
```{r}
result <- data %>%
  filter(duplicated(PARCELNO) | duplicated(PARCELNO, fromLast = TRUE)) %>%
  group_by(PARCELNO) %>%
  arrange(PARCELNO, month_sold) %>%
  mutate(
    max_month = max(month_sold),                    
    is_latest_month = month_sold == max_month,         
    is_price_higher_for_latest_month = SALE_PRC == max(SALE_PRC[is_latest_month]) 
  ) %>%
  ungroup() %>%
  filter(is_latest_month != is_price_higher_for_latest_month) 

print(result)
```

We keep the record with the later month sold for properties that appear multiple times.
```{r}
data <- data %>%
  group_by(PARCELNO) %>%
  filter(month_sold == max(month_sold)) %>%
  ungroup()

```
We check if there are any duplicates left.
```{r}
duplicate_rows <- data[duplicated(data$PARCELNO) | duplicated(data$PARCELNO, fromLast = TRUE), ]
duplicate_rows
```
As we can see, we now only have to handle those entries with duplicate PARCELNO where the month sold is also the same.
For these records we take the average sale price of both entries.
```{r}
aggregated_data <- data %>%
  group_by(PARCELNO) %>%
  summarise(
    SALE_PRC = mean(SALE_PRC, na.rm = TRUE),
    across(-SALE_PRC, ~ first(.))
  ) %>%
  ungroup()

```

```{r}
duplicate_rows <- aggregated_data[duplicated(aggregated_data$PARCELNO) | duplicated(aggregated_data$PARCELNO, fromLast = TRUE), ]
duplicate_rows
```
Now we have data that dosent have duplicated PARCELNO.
```{r}
data<-aggregated_data
```


#### Mapping LATITUDE & LONGTITUDE columns to Zipcodes

```{r}
options(tigris_use_cache = TRUE)

zctas <- zctas(cb = TRUE, year = 2016)
```
```{r}
df<-data 
df$LATITUDE <- as.numeric(df$LATITUDE)
df$LONGITUDE <- as.numeric(df$LONGITUDE)

df <- df %>% filter(!is.na(LATITUDE) & !is.na(LONGITUDE))

df_sf <- st_as_sf(df, coords = c("LONGITUDE", "LATITUDE"), crs = 4326)

zctas <- st_transform(zctas, st_crs(df_sf))
names(zctas)
```
```{r}

zip_code_column <- "ZCTA5CE10" 

df_with_zip <- st_join(df_sf, zctas[zip_code_column], left = TRUE)

df_with_zip <- df_with_zip %>% rename(ZIP_CODE = all_of(zip_code_column))

df_with_zip$geometry <- NULL
```
```{r}
unique(df_with_zip$ZIP_CODE) 
```

#### Mapping Zipcodes to more general Areas

```{r}
regions_list <- list(
  "Miami_Beach" = c("33139", "33140", "33141", "33154", "33160", "33180"),
  "Hialeah_Gardens" = c("33018"),
  "Hialeah" = c("33010", "33012", "33013", "33014", "33015", "33016"),
  "Opa_locka" = c("33054", "33055", "33056"),
  "Northeast" = c("33161", "33162", "33179", "33181", "33138", "33137", "33132"),
  "Coral_Gables" = c("33146"),
  "Coconut_Grove" = c("33133"),
  "Southwest_Miami" = c("33155", "33156", "33157", "33158", "33165", "33173",
                        "33175", "33176", "33177", "33183", "33186", "33187",
                        "33189", "33193", "33196", "33194", "33184", "33144",
                        "33174", "33134", "33135", "33145", "33185", "33143",
                        "33170", "33190"),
  "Homestead" = c("33030", "33031", "33032", "33033", "33034", "33035", "33039"),
  "Downtown_Miami" = c("33128", "33129", "33130", "33131"),
  "Key_Biscayne" = c("33149"),
  "NW_Miami" = c("33192", "33182", "33172", "33178", "33166", "33122", "33126",
                 "33169", "33167", "33168", "33147", "33150", "33142", "33127",
                 "33136", "33125")
)

zip_region_df <- data.frame(ZIP_CODE = character(), Region = character(), stringsAsFactors = FALSE)
for (region in names(regions_list)) {
  zips <- regions_list[[region]]
  temp_df <- data.frame(ZIP_CODE = zips, Region = region, stringsAsFactors = FALSE)
  zip_region_df <- rbind(zip_region_df, temp_df)
}

df_with_zip$ZIP_CODE <- as.character(df_with_zip$ZIP_CODE)
zip_region_df$ZIP_CODE <- as.character(zip_region_df$ZIP_CODE)

df_with_zip <- merge(df_with_zip, zip_region_df, by = "ZIP_CODE", all.x = TRUE)

df_with_zip$Region[is.na(df_with_zip$Region)] <- "Other"
```

#### Removing unneccessary columns

First we remove the identifier from the data . It is not nessecary when we want to give the data to the model.
```{r}
library(dplyr)
df_with_zip <- df_with_zip %>% dplyr::select(-PARCELNO)
```

#### Log Transform skewed distributed variables

As we notices before quite a few variables have very skewed distributions. We fix that by log-transforming them. Since we have mayn variables that are simialr in their sementics, e.g. distance to some POI, it nmakes sense to scale all of them, even if some do not have very skewed original distributions.
```{r}
variables_to_log_transform <- c("WATER_DIST", "TOT_LVG_AREA", "LND_SQFOOT", "SPEC_FEAT_VAL",
                                "RAIL_DIST", "OCEAN_DIST", "CNTR_DIST", "SUBCNTR_DI",
                                "HWY_DIST", "age","SALE_PRC")


for (var in variables_to_log_transform) {
  zeros <- sum(df_with_zip[[var]] == 0, na.rm = TRUE)
  negatives <- sum(df_with_zip[[var]] < 0, na.rm = TRUE)
  cat("Variable:", var, "- Zeros:", zeros, "- Negatives:", negatives, "\n")
}
```

```{r}
columns_to_transform <- c("WATER_DIST", "TOT_LVG_AREA", "LND_SQFOOT", "SPEC_FEAT_VAL",
                          "RAIL_DIST", "OCEAN_DIST", "CNTR_DIST", "SUBCNTR_DI",
                          "HWY_DIST", "age","SALE_PRC")

for (col in columns_to_transform) {

  if (any(df_with_zip[[col]] <= 0, na.rm = TRUE)) {
   
    df_with_zip[[col]] <- log1p(df_with_zip[[col]]) #for the columns with 0 values we use log(x+1)
  } else {
   
    df_with_zip[[col]] <- log(df_with_zip[[col]])
  }
}
head(df_with_zip)
```
We plot the distribution histograms again to see if it worked.
```{r}
for (col in columns_to_transform) {
  hist(df_with_zip[[col]], 
       breaks = 30, 
       main = paste("Histogram of Log-Transformed", col), 
       xlab = paste("Log(", col, ")", sep = ""), 
       col = "skyblue")
}

new_data_dummy <- df_with_zip
```

#### Scaling numerical Columns

Now we want to scale the numerical columns. Many machine learning models work better when features are on a similar scale. Since many of R's functions already to scaling internally, we wouldn't actually need to do this step. We do it here as a demonstration how to, but continue with the unscaled data.
```{r}
scaled_data <- df_with_zip
str(scaled_data)

numerical_cols <- c("LND_SQFOOT", "TOT_LVG_AREA", "SPEC_FEAT_VAL", "RAIL_DIST",
                    "OCEAN_DIST", "WATER_DIST", "CNTR_DIST", "SUBCNTR_DI",
                    "HWY_DIST", "age")
scaled_data[numerical_cols] <- scale(scaled_data[numerical_cols])

```

#### Encoding categorical columns

We now need to encode categorical columns since many ML methods cannot use them. The noise dummy is already encoded and the structure quality grade is also ordinal on an integer scale, which models can work with. Therefore we only need to encode the mapped Regions by using One-Hot encoding.
```{r}
df_with_zip <- dummy_cols(df_with_zip, select_columns = "Region", remove_first_dummy = TRUE, remove_selected_columns = TRUE)
```

Now we have our final data.
```{r}
data_clean <- df_with_zip
data_clean <- data_clean %>% dplyr::select(-ZIP_CODE)
new_data_dummy <- new_data_dummy %>% dplyr::select(-ZIP_CODE)
new_data_dummy$SALE_PRC <- exp(new_data_dummy$SALE_PRC)
```

## 4

```{r}
set.seed(1234)
n <- nrow(data_clean)
train_valid <- sample(1:n, round((0.8) * n))
test<-(1:n)[-train_valid]

n_tv <- length(train_valid)
train <- sample(1:n_tv, round((0.8) * n_tv))
valid <- (1:n_tv)[-train]
```

```{r}
model_lm <- lm(SALE_PRC ~., data = data_clean, subset = train)
summary(model_lm)
```

```{r}
resid <- exp(residuals(model_lm))
n_train <- length(train)

S <- (1/n_train)*sum(resid)

valid_pred <- exp((predict(model_lm, newdata=data_clean[valid,])))*S

valid_y <- exp(data_clean[valid,"SALE_PRC"])
```



```{r}


plot(valid_y, valid_pred)
abline(0, 1, col = "red", lwd = 2)

n_valid <- length(valid)

RMSE_valid <- sqrt((1/n_valid)*sum((valid_y - valid_pred)^2))
print(RMSE_valid)
```


```{r}
library(glmnet)
model_ridge <- glmnet(as.matrix(data_clean[train,-1]),data_clean[train,1],alpha=0)
```
```{r}
plot(model_ridge, xvar="lambda")
```

```{r}
ridge_cv <- cv.glmnet(as.matrix(data_clean[train,-1]),data_clean[train,1],alpha=0)
```

```{r}
plot(ridge_cv)
```
```{r}
valid_pred_ridge <- predict(ridge_cv,newx=as.matrix(data_clean[valid,-1]),s="lambda.1se")

plot(valid_y, exp(valid_pred_ridge))
abline(0, 1, col = "red", lwd = 2)

n_valid <- length(valid)

RMSE_valid <- sqrt((1/n_valid)*sum((valid_y - exp(valid_pred_ridge))^2))
print(RMSE_valid)
```

```{r}
library(glmnet)
model_lasso <- glmnet(as.matrix(data_clean[train,-1]),data_clean[train,1],alpha=1)
```
```{r}
plot(model_lasso, xvar="lambda")
```

```{r}
lasso_cv <- cv.glmnet(as.matrix(data_clean[train,-1]),data_clean[train,1],alpha=1)
```

```{r}
plot(lasso_cv)
```
```{r}
valid_pred_lasso <- predict(lasso_cv,newx=as.matrix(data_clean[valid,-1]),s="lambda.1se")




plot(exp(valid_y), exp(pred_exp))
abline(0, 1, col = "red", lwd = 2)

n_valid <- length(valid)

RMSE_valid <- sqrt(sum((exp(valid_y) - exp(pred_exp))^2)*1/n_valid)
print(RMSE_valid)
```


```{r}
str(valid_pred_lasso)
str(s2)
```


```{r}
data_exp <- data_clean
data_exp[,"SALE_PRC"] <- exp(data_exp[,"SALE_PRC"])


```

```{r}
model_lm_exp <- lm(SALE_PRC~.,data=data_exp,subset = train)
```
```{r}
library(MASS)

bc <- boxcox(model_lm_exp)

lambda_opt <- bc$x[which.max(bc$y)]

lambda_opt
y_transformed <- (data_exp$SALE_PRC^lambda_opt - 1)/lambda_opt

lm_box <- lm(y_transformed~.,data=data_exp[,-1],subset = train)

pred_trans <- predict(lm_box, newdata = data_exp[valid, -1])

pred_original <- ((pred_trans * lambda_opt) + 1)^(1 / lambda_opt)

plot(data_exp[valid,]$SALE_PRC, pred_original)
abline(0, 1, col = "red", lwd = 2)

rmse <- sqrt(mean((data_exp[valid,]$SALE_PRC - pred_original)^2))
rmse
```








```{r}
library(glmnet)
lasso_cv <- cv.glmnet(as.matrix(data_exp[train,-1]),data_exp[train,1],alpha=1)
```

```{r}
plot(lasso_cv)
```
```{r}
valid_pred_lasso <- predict(lasso_cv,newx=as.matrix(data_exp[valid,-1]),s="lambda.1se")


valid_y_exp <- data_exp[valid,1]

plot(valid_y_exp, valid_pred_lasso)
abline(0, 1, col = "red", lwd = 2)

n_valid <- length(valid)

RMSE_valid <- sqrt(sum((valid_y_exp - valid_pred_lasso)^2)*1/n_valid)
print(RMSE_valid)
```
```{r}
model_log <- glm(SALE_PRC~.,data=data_exp,subset=train)

log_pred <- predict(model_log,newdata=data_exp[valid,])

plot(valid_y_exp,log_pred)
abline(0, 1, col = "red", lwd = 2)
```


```{r}
library(randomForest)

rf_model <- randomForest(SALE_PRC ~ ., data = data_exp[train,], ntree = 100)
pred_rf <- predict(rf_model, newdata = data_exp[valid,])

plot(data_exp$SALE_PRC[valid], pred_rf)
abline(0,1,col="red")

rmse <- sqrt(mean((data_exp$SALE_PRC[valid] - pred_rf)^2))
rmse
```


```{r}
library(quantregForest)

data_exp_train <- data_exp[train,-1]
data_exp_valid <- data_exp[valid,-1]
sale_exp_train <- data_exp[train,1]

qrf_model <- quantregForest(y = sale_exp_train, x = data_exp_train, ntree = 500)
pred_med <- predict(qrf_model, newdata = data_exp_valid, what = 0.5)

plot(data_exp$SALE_PRC[valid], pred_med)
abline(0,1,col="red")
```

```{r}
library(xgboost)

x_train <- model.matrix(~ . - SALE_PRC, data = data_exp[train, ])
x_valid <- model.matrix(~ . - SALE_PRC, data = data_exp[valid, ])

y_train <- data_exp$SALE_PRC[train]
y_valid <- data_exp$SALE_PRC[valid]

dtrain <- xgb.DMatrix(data = x_train, label = y_train)
dvalid <- xgb.DMatrix(data = x_valid, label = y_valid)

params <- list(
  objective = "reg:squarederror",  # for regression
  eta = 0.1,                       # learning rate
  max_depth = 6,                   # depth of each tree
  subsample = 0.8,                 # subsample ratio of the training set
  colsample_bytree = 0.8           # subsample ratio of columns when constructing each tree
)

set.seed(123)
xgb_model <- xgb.train(
  params = params,
  data = dtrain,
  nrounds = 500,              # number of boosting rounds (trees)
  watchlist = list(train = dtrain, test = dvalid),
  early_stopping_rounds = 50, # stop if no improvement for 50 rounds
  print_every_n = 50
)

pred_xgb <- predict(xgb_model, newdata = dvalid)

plot(y_valid, pred_xgb)
abline(0,1,col="red")

rmse <- sqrt(mean((y_valid - pred_xgb)^2))
rmse
```

```{r}
threshold <- 700000
new_data_dummy$high_end_flag <- ifelse(new_data_dummy$SALE_PRC > threshold, "High", "Not_High")
new_data_dummy$high_end_flag <- factor(new_data_dummy$high_end_flag)

# First, train a classifier to predict if a home is high-end or not
classifier <- randomForest(high_end_flag ~ . - SALE_PRC, data = new_data_dummy[train,])

# Predict on valid set
high_end_pred <- predict(classifier, newdata = new_data_dummy[valid,], type = "response")

valid_high <- new_data_dummy[valid,][high_end_pred == "High", ]
valid_not_high <- new_data_dummy[valid,][high_end_pred == "Not_High", ]

high_price_df <- subset(new_data_dummy[train, ], SALE_PRC > threshold)

# Train separate regression models
# Model for not-high properties
rf_not_high <- randomForest(SALE_PRC ~ ., data = subset(new_data_dummy[train,], SALE_PRC <= threshold))

str(high_price_df)

hallo <- high_price_df[,-12]
hallo <- hallo[,-15]
# Regression for high
rf_high <- lm(SALE_PRC ~ ., data = hallo)

pred_not_high <- predict(rf_not_high, newdata = valid_not_high)
pred_high <- predict(rf_high, newdata = valid_high)

# Combine predictions
pred_combined <- numeric(length(valid))
pred_combined[high_end_pred == "Not_High"] <- pred_not_high
pred_combined[high_end_pred == "High"] <- pred_high
```
```{r}
plot(data_exp$SALE_PRC[valid], pred_combined)
abline(0,1,col="red")

rmse <- sqrt(mean((data_exp$SALE_PRC[valid] - pred_combined)^2))
rmse
```
```

